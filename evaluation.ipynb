{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from deepeval import evaluate\n",
    "from deepeval.models import GeminiModel\n",
    "from deepeval.evaluate import AsyncConfig\n",
    "from deepeval.evaluate import ErrorConfig\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, ContextualRelevancyMetric\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from src.text.vector import VectorStore\n",
    "from src.model.model import MultiModalEmbeddingModel, OllamaLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1ec35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Actual Outputs Generatation (with RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "print(os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "model = GeminiModel(\n",
    "    model_name=\"gemini-2.0-flash\",\n",
    "    api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9632d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = {}\n",
    "with open(os.path.join(os.getcwd(), \"dataset\", \"generated.json\"), \"r\") as f:\n",
    "    generated = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "queries_path = os.path.join(os.getcwd(), \"dataset\", \"dataset.json\")\n",
    "\n",
    "with open(queries_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    for k, v in data.items():\n",
    "        if v[\"query\"] != \"\" and v[\"golden\"] != \"\":\n",
    "            queries.append((k, v[\"query\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623fae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate = True\n",
    "\n",
    "if len(queries) == len(generated):\n",
    "    print(\"All queries have been generated\")\n",
    "    generate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate:\n",
    "    vector_store = VectorStore(\n",
    "        embed_model=MultiModalEmbeddingModel(\"nomic-ai/nomic-embed-text-v1.5\", \"nomic-ai/nomic-embed-vision-v1.5\"),\n",
    "        persist_directory=\"eval\")\n",
    "    local_model = OllamaLanguageModel(model_name=\"gemma3:27b-it-qat\", temperature=0.0).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62deb34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "root_folder = os.path.join(os.getcwd(), \"dataset\", \"text\")\n",
    "\n",
    "for file in os.listdir(root_folder):\n",
    "    file_path = os.path.join(root_folder, file)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        text = f.read()\n",
    "    texts.append(text)\n",
    "\n",
    "if generate:\n",
    "    vector_store.add(texts)\n",
    "    print(f\"{len(vector_store.vector_store.get()['ids'])} documents loaded to vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e631b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(query, llm, vector_store):\n",
    "    def format_docs(docs):\n",
    "        context = \"\"\n",
    "        for doc in docs:\n",
    "            context += f\"{doc.page_content}\\n\\n\"\n",
    "        return context\n",
    "\n",
    "    system_prompt = '''\n",
    "        # Role\n",
    "        \n",
    "        You are an expert AI professor capable of summarizing classroom materials to make understand better the concepts. In particular you will be answering user queries based on retrieved information from a database of classroom materials.\n",
    "        \n",
    "        ## Input:\n",
    "\n",
    "            - Context: Revelevant chunks retrieved from the database of classroom materials.\n",
    "            - User Query: User question.\n",
    "\n",
    "        ## Instruction:\n",
    "\n",
    "            1. Answer only using the provided context. If the context contains sufficient information to answer the query, provide a precise, well-structured response, without referring to general knowledge or external sources.\n",
    "            2. If you don't know what to say, just say that you don't know.\n",
    "            3. Answer in English.\n",
    "        '''\n",
    "\n",
    "    user_prompt = '''\n",
    "        # Inputs\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        User Query:\n",
    "        {query}\n",
    "        '''\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"user\", user_prompt),\n",
    "        ])\n",
    "\n",
    "    retriever = vector_store.get_retriever(filter={\"type\": \"text\"})\n",
    "    docs = retriever.invoke(query)\n",
    "    retrieved_context = format_docs(docs)\n",
    "    \n",
    "    rag_chain = (\n",
    "        prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    output = rag_chain.invoke({\"context\": retrieved_context,\n",
    "                                \"query\": query})\n",
    "    docs = [doc.page_content for doc in docs]\n",
    "    return output, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2334bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "retrieved_contexts = []\n",
    "\n",
    "if generate:\n",
    "    for i, query in enumerate(queries):\n",
    "        query = query[1]\n",
    "        output, retrieved_context = inference(query, local_model, vector_store)\n",
    "        outputs.append(output)\n",
    "        retrieved_contexts.append(retrieved_context)\n",
    "else:\n",
    "    for query in queries:\n",
    "        query = query[0]\n",
    "        output = generated[query][\"output\"]\n",
    "        retrieved_context = generated[query][\"retrieved_context\"]\n",
    "        outputs.append(output)\n",
    "        retrieved_contexts.append(retrieved_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e4f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate:\n",
    "    generated = {}\n",
    "\n",
    "    for query, output, retrieved_context_list in zip(queries, outputs, retrieved_contexts):\n",
    "        key = query[0]\n",
    "        generated[key] = {\n",
    "            \"query\": query[1],\n",
    "            \"output\": output,\n",
    "            \"retrieved_context\": retrieved_context_list\n",
    "        }\n",
    "\n",
    "    with open(os.path.join(os.getcwd(), \"dataset\", \"generated.json\"), \"w\") as f:\n",
    "        json.dump(generated, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f46b723",
   "metadata": {},
   "source": [
    "---\n",
    "# Rag Triad Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba597107",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = []\n",
    "\n",
    "for query, output, retrieved_contexts in zip(queries, outputs, retrieved_contexts):\n",
    "    test_case = LLMTestCase(\n",
    "        input=query[1],\n",
    "        actual_output=output,\n",
    "        retrieval_context=retrieved_contexts,\n",
    "    )\n",
    "    test_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e597ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevancy = AnswerRelevancyMetric(model=model, async_mode=False)\n",
    "faithfulness = FaithfulnessMetric(model=model, async_mode=False)\n",
    "contextual_relevancy = ContextualRelevancyMetric(model=model, async_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ecd311",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_config = AsyncConfig(\n",
    "    run_async=False,\n",
    "    max_concurrent=1,\n",
    "    throttle_value=1\n",
    "    )\n",
    "\n",
    "error_config = ErrorConfig(\n",
    "    ignore_errors=True,\n",
    ")\n",
    "\n",
    "results = evaluate(\n",
    "    test_cases=test_cases,\n",
    "    metrics=[\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        contextual_relevancy\n",
    "    ],\n",
    "    async_config=async_config,\n",
    "    error_config=error_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82abe7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevancies = []\n",
    "faithfulnesses = []\n",
    "contextual_relevancies = []\n",
    "\n",
    "for test in results.test_results:\n",
    "    for i, metric in enumerate(test.metrics_data):\n",
    "        if i == 0:\n",
    "            relevancies.append(metric.score)\n",
    "        elif i == 1:\n",
    "            faithfulnesses.append(metric.score)\n",
    "        elif i == 2:\n",
    "            contextual_relevancies.append(metric.score)\n",
    "\n",
    "relevance = sum(relevancies) / len(relevancies)\n",
    "faithfulness = sum(faithfulnesses) / len(faithfulnesses)\n",
    "contextual_relevancy = sum(contextual_relevancies) / len(contextual_relevancies)\n",
    "\n",
    "print(f\"Relevancy: {relevance:2f}\")\n",
    "print(f\"Faithfulness: {faithfulness:2f}\")\n",
    "print(f\"Contextual Relevancy: {contextual_relevancy:2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22efbedd",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Standard Evaluation: BERT Score - BLEU - ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "goldens = {}\n",
    "dataset_path = os.path.join(os.getcwd(), \"dataset\", \"dataset.json\")\n",
    "\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    for k, v in data.items():\n",
    "        if v[\"query\"] != \"\" and v[\"golden\"] != \"\":\n",
    "            goldens[k] = v[\"golden\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66af4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "\n",
    "for k, v in generated.items():\n",
    "    final_data.append({\n",
    "        \"golden\": goldens[k],\n",
    "        \"output\": v[\"output\"],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU code taken from: https://github.com/confident-ai/deepeval/blob/main/deepeval/scorer/scorer.py#L52\n",
    "\n",
    "def sentence_bleu_score(references, prediction, bleu_type = \"bleu1\"):\n",
    "    \"\"\"Calculates the BLEU (Bilingual Evaluation Understudy) score for a given prediction compared to one or more reference sentences.\n",
    "\n",
    "    BLEU is a metric used to evaluate the quality of machine-generated text by comparing it to one or more reference sentences.\n",
    "    It measures the similarity of the generated text to the reference text based on n-grams.\n",
    "\n",
    "    Args:\n",
    "        references (Union[str, List[str]): A reference sentence or a list of reference sentences.\n",
    "        prediction (str): The generated text or sentence to be evaluated.\n",
    "        bleu_type (Optional[str]): The BLEU score type (Options: 'bleu1', 'bleu2', 'bleu3', 'bleu4'). Default is 'bleu1'.\n",
    "\n",
    "    Returns:\n",
    "        float: The BLEU score for the given prediction and references.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        from nltk.translate.bleu_score import sentence_bleu\n",
    "    except ModuleNotFoundError as e:\n",
    "        print(\"Please install nltk module. Command: pip install nltk\")\n",
    "\n",
    "    assert bleu_type in [\n",
    "        \"bleu1\",\n",
    "        \"bleu2\",\n",
    "        \"bleu3\",\n",
    "        \"bleu4\",\n",
    "    ], \"Invalid bleu_type. Options: 'bleu1', 'bleu2', 'bleu3', 'bleu4'\"\n",
    "    targets = [references] if isinstance(references, str) else references\n",
    "    tokenized_targets = [word_tokenize(target) for target in targets]\n",
    "    tokenized_prediction = word_tokenize(prediction)\n",
    "    bleu_weight_map = {\n",
    "        \"bleu1\": (1, 0, 0, 0),\n",
    "        \"bleu2\": (0, 1, 0, 0),\n",
    "        \"bleu3\": (0, 0, 1, 0),\n",
    "        \"bleu4\": (0, 0, 0, 1),\n",
    "    }\n",
    "    return sentence_bleu(\n",
    "        tokenized_targets,\n",
    "        tokenized_prediction,\n",
    "        weights=bleu_weight_map[bleu_type],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE code taken from: https://github.com/confident-ai/deepeval/blob/main/deepeval/scorer/scorer.py#L19\n",
    "\n",
    "def rouge_score(target: str, prediction: str, score_type: str) -> float:\n",
    "        \"\"\"Calculates the Rouge score for a given target and prediction.\n",
    "\n",
    "        Rouge (Recall-Oriented Understudy for Gisting Evaluation) is a metric used for evaluating the quality of generated text,\n",
    "        especially in tasks like text summarization.\n",
    "\n",
    "        To utilize the rouge_score scoring method, be sure to `pip install rouge-score` before calling this method.\n",
    "\n",
    "        Args:\n",
    "            target (str): The actual label or target text.\n",
    "            prediction (str): The generated text from the model or LLM.\n",
    "            score_type (str): The Rouge score type (Options: 'rouge1', 'rouge2', 'rougeL').\n",
    "\n",
    "        Returns:\n",
    "            float: The Rouge score for the given target and prediction, based on the specified score type.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from rouge_score import rouge_scorer\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        assert score_type in [\n",
    "            \"rouge1\",\n",
    "            \"rouge2\",\n",
    "            \"rougeL\",\n",
    "        ], \"score_type can be either rouge1, rouge2 or rougeL\"\n",
    "        scorer = rouge_scorer.RougeScorer([score_type], use_stemmer=True)\n",
    "        scores = scorer.score(target, prediction)\n",
    "        return scores[score_type].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc1af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTScore Taken from: https://github.com/confident-ai/deepeval/blob/main/deepeval/scorer/scorer.py#L129\n",
    "\n",
    "def bert_score(references, predictions, model = \"microsoft/deberta-large-mnli\", lang = \"en\") -> float:\n",
    "    \"\"\"\n",
    "    Calculate BERTScore for one or more reference sentences compared to one or more prediction sentences using a specified BERT model.\n",
    "\n",
    "    Args:\n",
    "        references (Union[str, List[str]]): A single reference sentence or a list of reference sentences.\n",
    "        predictions (Union[str, List[str]]): A single prediction sentence or a list of prediction sentences.\n",
    "        model (Optional[str], optional): The name of the BERT model to be used for scoring. Defaults to \"microsoft/deberta-large-mnli\".\n",
    "        lang (Optional[str], optional): The language code of the text, e.g., \"en\" for English. Defaults to \"en\".\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary containing BERTScore metrics including precision, recall, and F1 score.\n",
    "            - 'bert-precision' (float): BERTScore precision.\n",
    "            - 'bert-recall' (float): BERTScore recall.\n",
    "            - 'bert-f1' (float): BERTScore F1 score.\n",
    "\n",
    "    Note:\n",
    "        Before using this function, make sure to install the 'bert_score' module by running the following command:\n",
    "        ```\n",
    "        pip install bert-score\n",
    "        ```\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from bert_score import BERTScorer\n",
    "    except ModuleNotFoundError as e:\n",
    "        print(\n",
    "            \"Please install bert_score module. Command: pip install bert-score\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "    except ModuleNotFoundError as e:\n",
    "        print(\"Please install torch module. Command: pip install torch\")\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    bert_scorer = BERTScorer(\n",
    "        model_type=model,\n",
    "        lang=lang,\n",
    "        rescale_with_baseline=True,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    if isinstance(predictions, str):\n",
    "        predictions = [predictions]\n",
    "\n",
    "    if isinstance(references, str):\n",
    "        references = [references]\n",
    "\n",
    "    if (\n",
    "        isinstance(predictions, list)\n",
    "        and isinstance(references, list)\n",
    "        and not isinstance(references[0], list)\n",
    "    ):\n",
    "        if len(predictions) != len(references):\n",
    "            references = [references]\n",
    "\n",
    "    precision, recall, f1 = bert_scorer.score(\n",
    "        cands=predictions, refs=references\n",
    "    )\n",
    "    return {\n",
    "        \"bert-precision\": precision.detach().numpy().tolist(),\n",
    "        \"bert-recall\": recall.detach().numpy().tolist(),\n",
    "        \"bert-f1\": f1.detach().numpy().tolist(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4ea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40bd99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleus = []\n",
    "rouges = []\n",
    "\n",
    "for couple in final_data:\n",
    "    res = sentence_bleu_score(couple[\"golden\"], couple[\"output\"], bleu_type=\"bleu4\")\n",
    "    bleus.append(res)\n",
    "    \n",
    "    res = rouge_score(couple[\"golden\"], couple[\"output\"], score_type=\"rougeL\")\n",
    "    rouges.append(res)\n",
    "\n",
    "references = [elem[\"golden\"] for elem in final_data]\n",
    "predictions = [elem[\"output\"] for elem in final_data]\n",
    "bert_scores = bert_score(references, predictions, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b641d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = sum(bleus) / len(bleus)\n",
    "rouge = sum(rouges) / len(rouges)\n",
    "bert_precision = sum(bert_scores[\"bert-precision\"]) / len(bert_scores[\"bert-precision\"])\n",
    "bert_recall = sum(bert_scores[\"bert-recall\"]) / len(bert_scores[\"bert-recall\"])\n",
    "bert_f1 = sum(bert_scores[\"bert-f1\"]) / len(bert_scores[\"bert-f1\"])\n",
    "\n",
    "\n",
    "print(f\"BLEU: {bleu:5f}\")\n",
    "print(f\"ROUGE: {rouge:5f}\")\n",
    "print(f\"BERT Precision: {bert_precision:2f}\")\n",
    "print(f\"BERT Recall: {bert_recall:2f}\")\n",
    "print(f\"BERT F1: {bert_f1:2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QuizCraft-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
